{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "type=<class 'spacy.tokens.span.Span'>, n_words=21, text=When I want to split a sentence, I apply my custom subsentencizer and wait for the results to be yielded\n",
      "type=<class 'spacy.tokens.span.Span'>, n_words=6, text=wait for the results to be\n"
     ]
    }
   ],
   "source": [
    "# %%writefile 'single_sentence.py'\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Set, List, Iterable\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from spacy.tokens.doc import Doc\n",
    "from spacy.tokens.span import Span\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SingleSentence(object):\n",
    "    root: Token\n",
    "    subtree: Set[spacy.tokens.token.Token]\n",
    "    \n",
    "    @property\n",
    "    def start_idx(self):\n",
    "        \"\"\"\n",
    "        Returns position (index) in original document where the single sentence begins.\n",
    "        \"\"\"\n",
    "        return min(self.subtree, key=lambda token: token.i).i\n",
    "    \n",
    "    @property\n",
    "    def end_index(self):\n",
    "        \"\"\"\n",
    "        Returns last position (index) of the single sentence in original document.\n",
    "        \"\"\"\n",
    "        return max(self.subtree, key=lambda token: token.i).i\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        \"\"\"\n",
    "        Return list of tokens.\n",
    "        \"\"\"\n",
    "        return sorted(list(self.subtree), key=lambda token: token.idx)\n",
    "\n",
    "\n",
    "def default_root_finding_strategy(token: Token) -> bool:\n",
    "    return (token.pos_ in {'VERB', 'ADJ'} and token.dep_ in {'ccomp', 'conj'}) or token.dep_=='ROOT'\n",
    "\n",
    "\n",
    "class SingleSentenceSplitter(object):\n",
    "    \"\"\"\n",
    "    Splits complex sentences into single-verb sentences using provided root-finding strategy.\n",
    "    Strategy is a function that, given a Token, shou\n",
    "    ld return True if this token is a root of a sentence.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_finding_strategy: Callable[[Token], bool]=default_root_finding_strategy):\n",
    "        self.root_finding_strategy = root_finding_strategy\n",
    "\n",
    "    def _get_single_sentences(self, sent: Span) -> List[SingleSentence]:\n",
    "        \"\"\"\n",
    "        Find roots of possible single sentences given the root finding strategy.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            SingleSentence(token, set(token.subtree))\n",
    "            for token in sent \n",
    "            if self.root_finding_strategy(token)\n",
    "        ]\n",
    "\n",
    "    def _make_unique(self, single_sents: List[SingleSentence]) -> List[SingleSentence]:\n",
    "        \"\"\"\n",
    "        Remove subsentences from their parent sentences so that each word is mapped \n",
    "        to one and only one single sentence.\n",
    "        \"\"\"\n",
    "        for i, s1 in enumerate(single_sents):\n",
    "            for j, s2 in enumerate(single_sents[:i]):\n",
    "                assert(j<i)\n",
    "                #if s1.subtree.issubset(s2.subtree):\n",
    "                if s1.root in s2.subtree:\n",
    "                    single_sents[j].subtree = single_sents[j].subtree - s1.subtree\n",
    "                #if s2.subtree.issubset(s1.subtree):\n",
    "                if s2.root in s1.subtree:\n",
    "                    single_sents[i].subtree = single_sents[i].subtree - s2.subtree\n",
    "        return single_sents\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Iterable[Span]:\n",
    "        \"\"\"\n",
    "        Perform sentence splitting on a given document.\n",
    "        \"\"\"\n",
    "        for sent in doc.sents:\n",
    "            separated_sents = self._make_unique(self._get_single_sentences(doc))\n",
    "            for single_sent in separated_sents:\n",
    "                #yield doc[single_sent.start_idx:single_sent.end_index]\n",
    "                yield single_sent.tokens\n",
    "\n",
    "\n",
    "\n",
    "def run_example():\n",
    "    \"\"\" \n",
    "    Usage example.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(\"When I want to split a sentence, I apply my custom subsentencizer and wait for the results to be yielded.\")\n",
    "    sss = SingleSentenceSplitter()  # pass custom root finding strategy here if necessary\n",
    "    for single_sent in sss(doc):\n",
    "        print(f\"type={type(single_sent)}, n_words={len(single_sent)}, text={single_sent}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
