{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP Glossary\n",
    "\n",
    "Overview of most common terms related to NLP, focusing on those related to today's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The process of splitting text into **tokens**.\n",
    "\n",
    "Tokens are parts of the text that may in some context have some meaning.\n",
    "Some of the most obvious tokens are:\n",
    "- words\n",
    "- punctuation\n",
    "- emojis\n",
    "\n",
    "Tokenization is a simple process, and for most languages can be performed using simple rules,\n",
    "although there are differences between languages - most notable of them shortcuts and multi-word names.\n",
    "\n",
    "SpaCy uses the same set of rules for all languages but allows them to add custom exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, 'm, in, ðŸ’™, with, N.Y., :)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I'm in ðŸ’™ with N.Y. :)\")\n",
    "print(list(doc))  # doc = sequence of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Sometimes, it is important to know the base form of a word (token),\n",
    "which is the thing you would find in a language.\n",
    "\n",
    "Knowing this base form can help with use cases such as:\n",
    "- counting **word frequency** (how many times each word appears in the text)\n",
    "- computing likelihood of two words being in one sentence\n",
    "\n",
    "We will discuss why lemmatization is important later in this course, for now let's just remember that it is there.\n",
    "\n",
    "As for the implementation, it is a much harder task than tokenization and requires much more information as an input. Luckily, SpaCy is our friend and gives us easy access to all tokens' lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple -> apple\n",
      "is -> be\n",
      "looking -> look\n",
      "buying -> buy\n",
      "U.K. -> u.k.\n",
      "startups -> startup\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startups for a total of $1 billion\")\n",
    "for token in doc:\n",
    "    if not token.text == token.lemma_:\n",
    "        print(f\"{token} -> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "While most of the tokens have some meaning, there are some of them that don't.\n",
    "\n",
    "In particular, words that appear very often often do not carry any meaning at all,\n",
    "you can think of them like a syntax sugar for a natural language to make it prettier.\n",
    "These words are called **stop words**.\n",
    "\n",
    "Whenever we are preparing to apply statistical methods (like any ML models) to natural language,\n",
    "it is worth removing all stop words as they are just an unnecessary noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![English Word Frequency](http://robslink.com/SAS/democd82/word_frequency.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(set([\"the\", \"of\", \"and\", \"to\", \"in\", \"a\"]) - spacy.lang.en.stop_words.STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
